{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'BasicBrowser'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fce4a57e7866>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# alternatively\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#settings.configure(DEBUG=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mdjango\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# import from appended path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/site-packages/django/__init__.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(set_prefix)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mdjango\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfigure_logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mconfigure_logging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOGGING_CONFIG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOGGING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mset_prefix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         set_script_prefix(\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/site-packages/django/conf/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;34m\"\"\"Return the value of a setting and cache it in self.__dict__.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/site-packages/django/conf/__init__.py\u001b[0m in \u001b[0;36m_setup\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 % (desc, ENVIRONMENT_VARIABLE))\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSettings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/site-packages/django/conf/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, settings_module)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSETTINGS_MODULE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msettings_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSETTINGS_MODULE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         tuple_settings = (\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'BasicBrowser'"
     ]
    }
   ],
   "source": [
    "# parser for pdf scans inside xml-files from German Bundestag / Opendata for periods 1 - 18\n",
    "# into the django parliament app\n",
    "# https://www.bundestag.de/services/opendata\n",
    "\n",
    "from __future__ import print_function\n",
    "import os, sys\n",
    "import django\n",
    "from django.conf import settings\n",
    "import re\n",
    "import requests\n",
    "import dataset\n",
    "import datetime\n",
    "from xml.etree import ElementTree\n",
    "from urllib.parse import urljoin\n",
    "# Extract agenda numbers not part of normdatei\n",
    "from normality import normalize\n",
    "import platform\n",
    "import zipfile\n",
    "\n",
    "if platform.node() == \"srv-mcc-apsis\":\n",
    "    sys.path.append('/home/leey/tmv/BasicBrowser/')\n",
    "    data_dir = '/home/leey/Plenarprotokolle'\n",
    "else:\n",
    "    # local paths\n",
    "    sys.path.append('/home/leey/Documents/Data/tmv/BasicBrowser/')\n",
    "    data_dir = '/home/leey/Documents/Data/Plenarprotokolle'\n",
    "\n",
    "# imports and settings for django and database\n",
    "# --------------------------------------------\n",
    "\n",
    "os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"BasicBrowser.settings\")\n",
    "# alternatively\n",
    "#settings.configure(DEBUG=True)\n",
    "django.setup()\n",
    "\n",
    "# import from appended path\n",
    "import parliament.models as pm\n",
    "from parliament.tasks import do_search\n",
    "import cities.models as cmodels\n",
    "\n",
    "from parsing_utils import dehyphenate, POI, clean_text, correct_pdf_parsing_errors, search_party_names\n",
    "from find_person_in_db_pdf import find_person_in_db\n",
    "from regular_expressions_global import *\n",
    "\n",
    "import pprint\n",
    "pretty_printer = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     8
    ]
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "# ============================================================\n",
    "# write output to file and terminal\n",
    "\n",
    "time_stamp = datetime.datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "output_file = \"./parlsessions_pdf_parser_output_\" + time_stamp + \".log\"\n",
    "print(\"log file: {}\".format(output_file))\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self):\n",
    "        self.terminal = sys.stdout\n",
    "        self.log = open(output_file, \"a\")\n",
    "\n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.log.write(message)\n",
    "\n",
    "    def flush(self):\n",
    "        #this flush method is needed for python 3 compatibility.\n",
    "        #this handles the flush command by doing nothing.\n",
    "        #you might want to specify some extra behavior here.\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     2,
     20,
     120,
     135,
     142,
     152,
     158
    ]
   },
   "outputs": [],
   "source": [
    "class SpeechParser(object):\n",
    "\n",
    "    def __init__(self, lines, verbosity=0):\n",
    "        self.lines = lines\n",
    "        self.line_number = 0\n",
    "        self.was_chair = True\n",
    "        self.date = None\n",
    "        self.verbosity = verbosity\n",
    "        self.in_session = False\n",
    "        self.in_header = False\n",
    "        self.in_poi = False\n",
    "        self.poi_content = \"\"\n",
    "        self.poi_linecounter = 0\n",
    "        self.chair = False\n",
    "        self.text = []\n",
    "        self.pars = []\n",
    "        self.speaker = None\n",
    "        self.speaker_party = None\n",
    "        self.warnings_counter = 0\n",
    "\n",
    "    def get_date(self):\n",
    "        for line in self.lines:\n",
    "            date_match = DATE.search(line)\n",
    "            if date_match:\n",
    "                try:\n",
    "                    d = int(date_match.group(2))\n",
    "                    m = int(D_MONTHS[date_match.group(3)])\n",
    "                    y = int(date_match.group(4))\n",
    "                    date = datetime.date(y, m, d)\n",
    "                    self.date = date\n",
    "                    return\n",
    "\n",
    "                except ValueError:\n",
    "                    print(\"date from manuscript not readable: {}\".format(DATE.match(line)))\n",
    "                    print(\"group 1: {}\".format(DATE.match(line).group(1)))\n",
    "                    print(\"group 2: {}\".format(DATE.match(line).group(2)))\n",
    "                    print(\"group 3: {}\".format(DATE.match(line).group(3)))\n",
    "                    print(\"group 4: {}\".format(DATE.match(line).group(4)))\n",
    "                    raise ValueError\n",
    "\n",
    "        print(\"Parser: Did not find date\")\n",
    "        return None\n",
    "\n",
    "    def append_text_and_poi(self):\n",
    "        par = {\n",
    "            'text': dehyphenate(self.text),\n",
    "            'pois': []\n",
    "        }\n",
    "        for poi_raw in re.split(' [-–—]-? ', self.poi_content):\n",
    "            poi_obj = POI(poi_raw)\n",
    "            par['pois'].append(poi_obj)\n",
    "            if self.verbosity > 0:\n",
    "                print(\"interjection: speakers: {}, party: {}, type: {},\"\n",
    "                      \"\\ninterjection text: {}\".format(poi_obj.speakers, poi_obj.parties, poi_obj.type, poi_obj.poitext))\n",
    "\n",
    "        self.pars.append(par)\n",
    "        self.text = []\n",
    "        self.poi_content = \"\"\n",
    "\n",
    "    def emit(self):\n",
    "        data = {\n",
    "            'speaker': self.speaker,\n",
    "            'speaker_party': self.speaker_party,\n",
    "            'type': 'chair' if self.chair else 'speech',\n",
    "            'pars': self.pars\n",
    "        }\n",
    "        self.was_chair = self.chair\n",
    "        self.text = []\n",
    "        self.pars = []\n",
    "        if self.verbosity > 1:\n",
    "            print(\"utterance: {}\".format(data))\n",
    "        return data\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        no_lines = len(self.lines)\n",
    "        self.line_number = -1\n",
    "\n",
    "        while self.line_number + 1 < no_lines:\n",
    "            self.line_number += 1\n",
    "            line = self.lines[self.line_number].strip()\n",
    "            if verbosity > 1:\n",
    "                print(\"- l{l:04d}: \".format(l=self.line_number) + line)\n",
    "\n",
    "            # Check if in session, session beginning, session ending\n",
    "            if not self.in_session and BEGIN_MARK.match(line):\n",
    "                print(\"= matched begin mark at line {}: {}\".format(self.line_number, line))\n",
    "                self.in_session = True\n",
    "                continue\n",
    "            if not self.in_session and INCOMPLETE_BEGIN_MARK.match(line):\n",
    "                print(\"! warning at line {}: Matched only incomplete begin mark: {}\".format(self.line_number, line))\n",
    "                self.warnings_counter += 1\n",
    "                self.in_session = True\n",
    "\n",
    "            elif not self.in_session:\n",
    "                continue\n",
    "\n",
    "            if DISRUPTION_MARK.match(line):\n",
    "                continue\n",
    "\n",
    "            for k in range(1,3):\n",
    "                lines = \"\\n\".join(self.lines[self.line_number:self.line_number+k])\n",
    "                lines = dehyphenate(lines)\n",
    "                if END_MARK.search(lines):\n",
    "                    print(\"= matched end mark at line {}: {}\".format(self.line_number, lines))\n",
    "                    self.text.append(lines)\n",
    "                    par = {\n",
    "                        'text': dehyphenate(self.text),\n",
    "                        # default for strip: removing leading and ending white space\n",
    "                        'pois': []\n",
    "                    }\n",
    "                    self.pars.append(par)\n",
    "                    yield self.emit()\n",
    "                    return\n",
    "\n",
    "            # empty line\n",
    "            if not len(line):\n",
    "                continue\n",
    "\n",
    "            # match repeated mentioning of speaker from header\n",
    "            if self.speaker:\n",
    "                speaker = self.speaker.replace('Dr. ', '')\n",
    "                speaker = speaker.split(\" (\")[0]\n",
    "                speaker = speaker.split(\", \")[0]\n",
    "                # print(\"looking for {}\".format(speaker))\n",
    "                try:\n",
    "                    SPEAKER_HEADER = re.compile('.{0,30}%s' % speaker)\n",
    "                    if SPEAKER_HEADER.match(line):\n",
    "                        if verbosity > 0:\n",
    "                            print(\"= matched speaker in header: \", line)\n",
    "                        continue\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            header_match = HEADER_MARK.match(line)\n",
    "            if header_match is not None:\n",
    "                if verbosity > 0:\n",
    "                    print(\"= matched header: \", line)\n",
    "\n",
    "                self.in_header = True\n",
    "                continue\n",
    "\n",
    "            if self.in_header and self.speaker is not None:\n",
    "                if line.startswith(self.speaker):\n",
    "                    if verbosity > 0:\n",
    "                        print(\"= matched current speaker in header: {}\".format(line))\n",
    "                    continue\n",
    "                else:\n",
    "                    self.in_header = False\n",
    "\n",
    "            is_top = False\n",
    "            # new point on the agenda (top - tagesordnungspunkt)\n",
    "            if TOP_MARK.match(line):\n",
    "                if verbosity > 0:\n",
    "                    print(\"= matched top mark: {}\".format(line))\n",
    "                is_top = True\n",
    "\n",
    "            has_stopword = False\n",
    "            for sw in SPEAKER_STOPWORDS:\n",
    "                if sw.lower() in line.lower():\n",
    "                    if verbosity > 0:\n",
    "                        print(\"= setting stopword flag\")\n",
    "                    has_stopword = True\n",
    "\n",
    "            noparty = False\n",
    "\n",
    "            for k in range(1,4):\n",
    "                lines = \"\\n\".join(self.lines[self.line_number:self.line_number+k])\n",
    "                lines = dehyphenate(lines, nl=True)\n",
    "                # print(repr(lines)) # print with escape characters\n",
    "                speaker_match = (PRESIDENT.match(lines) or\n",
    "                                 PARTY_MEMBER.match(lines) or\n",
    "                                 STAATSSEKR.match(lines) or\n",
    "                                 STAATSMINISTER.match(lines) or\n",
    "                                 WEHRBEAUFTRAGTER.match(lines) or\n",
    "                                 BUNDESKANZLER.match(lines) or\n",
    "                                 BEAUFTRAGT.match(lines) or\n",
    "                                 MINISTER.match(lines) or\n",
    "                                 BERICHTERSTATTER.match(lines))\n",
    "\n",
    "                if speaker_match is not None:\n",
    "                    if verbosity > 0:\n",
    "                        print(\"= matched speaker at line {}: {}\".format(self.line_number, speaker_match))\n",
    "                    self.in_poi = False\n",
    "                    self.line_number += k - 1\n",
    "                    break\n",
    "\n",
    "            if PARTY_MEMBER.match(line):\n",
    "                if not ANY_PARTY.match(normalize(PARTY_MEMBER.match(line).group(2))):\n",
    "                    if verbosity > 1:\n",
    "                        print(\"= {} could not be identified\".format(PARTY_MEMBER.match(line).group(2)))\n",
    "                        print(\"= set noparty flag\")\n",
    "                    noparty = True\n",
    "\n",
    "            if speaker_match is not None \\\n",
    "                    and not is_top \\\n",
    "                    and not noparty \\\n",
    "                    and not has_stopword:\n",
    "\n",
    "                if self.speaker is None and self.text == [] and self.pars == []:\n",
    "                    self.text = []\n",
    "                else:\n",
    "                    if verbosity > 1:\n",
    "                        print(\"number of paragraphs in utterance: {}\".format(len(self.pars)))\n",
    "                    if len(self.pars) < 1:\n",
    "                        par = {\n",
    "                            'text': dehyphenate(self.text),\n",
    "                            # default for strip: removing leading and ending white space\n",
    "                            'pois': []\n",
    "                        }\n",
    "                        self.pars.append(par)\n",
    "\n",
    "                    yield self.emit()\n",
    "\n",
    "                role = line.strip().split(' ')[0]\n",
    "                self.speaker = speaker_match.group(1)\n",
    "            #if PARTY_MEMBER_PDF.match(line):\n",
    "                #self.speaker_party = search_person_party(PARTY_MEMBER_PDF.match(line).group(3))\n",
    "            #else:\n",
    "                #self.speaker_party = search_person_party(line.strip().split(':')[0])\n",
    "                self.chair = role in CHAIRS\n",
    "                continue\n",
    "\n",
    "            poi_match = POI_MARK.match(line)\n",
    "            if poi_match is not None:\n",
    "                self.poi_content = poi_match.group(1)\n",
    "                self.append_text_and_poi()\n",
    "                continue\n",
    "\n",
    "            if not self.in_poi:\n",
    "                poi_begin = POI_BEGIN.match(line)\n",
    "                if poi_begin:\n",
    "                    if verbosity > 1:\n",
    "                        print(\"= raised in_poi flag\")\n",
    "                    self.in_poi = True\n",
    "                    self.poi_content = line\n",
    "                    self.poi_linecounter = 0\n",
    "                    continue\n",
    "            else:\n",
    "                self.poi_content += \"\\n\" + line\n",
    "                self.poi_linecounter += 1\n",
    "                if POI_END.match(line):\n",
    "                    self.poi_content = dehyphenate(self.poi_content).strip().strip('()')\n",
    "                    self.append_text_and_poi()\n",
    "                    if verbosity > 1:\n",
    "                        print(\"= matched poi end\")\n",
    "                    self.in_poi = False\n",
    "                if self.poi_linecounter > 10:\n",
    "                    print(\"! Warning: No match of poi end after 10 lines. Going back to normal mode.\")\n",
    "                    self.warnings_counter += 1\n",
    "                    self.in_poi = False\n",
    "                    self.text.append(self.poi_content)\n",
    "                continue\n",
    "\n",
    "            self.text.append(line)\n",
    "\n",
    "        print(\"! Warning: Reached end of file without end mark\")\n",
    "        self.warnings_counter += 1\n",
    "        yield self.emit()\n",
    "\n",
    "def file_metadata(filename):\n",
    "    fname = os.path.basename(filename)\n",
    "    try:\n",
    "        return int(fname[:2]), int(fname[2:5])\n",
    "    except:\n",
    "        return int(fname[:2]), fname[2:5]\n",
    "\n",
    "def german_date(str):\n",
    "    if str is None:\n",
    "        return None\n",
    "    return datetime.datetime.strptime(str,\"%d.%m.%Y\").date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     7,
     52,
     56,
     71,
     129,
     233
    ]
   },
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# ========== parse function ==========================================\n",
    "# ====================================================================\n",
    "\n",
    "def parse_transcript(file, verbosity=1):\n",
    "\n",
    "    warnings_counter2 = 0\n",
    "    if isinstance(file, str):\n",
    "        print(\"loading text from {}\".format(file))\n",
    "        try:\n",
    "            with open(file) as fh:\n",
    "                content = fh.read()\n",
    "                text = clean_text(content)\n",
    "        except UnicodeDecodeError:\n",
    "            print(\"Reloading in other encoding (windows-1252)\")\n",
    "            with open(file, encoding=\"windows-1252\") as fh:\n",
    "                content = fh.read()\n",
    "                text = clean_text(content)\n",
    "        filename = file\n",
    "        wp, session = file_metadata(filename)\n",
    "\n",
    "    # open file in zip archive\n",
    "    elif isinstance(file, zipfile.ZipExtFile):\n",
    "        content = file.read()\n",
    "        filename = file.name\n",
    "        if filename.endswith(\".xml\"):\n",
    "            root = ElementTree.fromstring(content)\n",
    "            if verbosity > 0:\n",
    "                print(\"loading text from {}\".format(filename))\n",
    "\n",
    "            # display contents of xml file\n",
    "            if verbosity > 1:\n",
    "                print(\"xml root: {}, attributes: {}\".format(root.tag, root.attrib))\n",
    "                for child in root:\n",
    "                    print(\"xml child: {}, attributes: {}\".format(child.tag, child.attrib))\n",
    "                    print(\"xml beginning of text: {}\".format(child.text[:100].replace('\\n', ' ')))\n",
    "\n",
    "            wp = int(root.find(\"WAHLPERIODE\").text)\n",
    "            document_type = root.find(\"DOKUMENTART\").text\n",
    "            if document_type != \"PLENARPROTOKOLL\":\n",
    "                print(\"Warning: document {} is not tagged as Plenarprotokoll but {}\".format(filename, document_type))\n",
    "                warnings_counter2 += 1\n",
    "            number = root.find(\"NR\").text\n",
    "            session = int(number.split(\"/\")[1])\n",
    "            date = root.find(\"DATUM\").text\n",
    "            titel = root.find(\"TITEL\").text\n",
    "            text = clean_text(root.find(\"TEXT\").text)\n",
    "            text = correct_pdf_parsing_errors(text)\n",
    "        else:\n",
    "            print(\"filetype not xml\")\n",
    "            return 0\n",
    "\n",
    "    else:\n",
    "        print(\"invalid filetype\")\n",
    "        return 0\n",
    "\n",
    "    base_data = {\n",
    "        'filename': filename,\n",
    "        'sitzung': session,\n",
    "        'wahlperiode': wp\n",
    "    }\n",
    "\n",
    "    print(\"\\nParsing transcript: {}/{}, from {}\".format(wp, session, filename))\n",
    "    utterance_counter = 0\n",
    "    paragraph_counter = 0\n",
    "    interjection_counter = 0\n",
    "\n",
    "    # start parsing\n",
    "    parser = SpeechParser(text.split('\\n'), verbosity=verbosity)\n",
    "    # get_date is not working for all documents\n",
    "    parser.get_date()\n",
    "    if isinstance(file, zipfile.ZipExtFile):\n",
    "        if parser.date != german_date(date):\n",
    "            print(\"! Warning: dates do not match\")\n",
    "            warnings_counter2 += 1\n",
    "            print(parser.date)\n",
    "            print(date)\n",
    "            print(german_date(date))\n",
    "\n",
    "    parl, created = pm.Parl.objects.get_or_create(\n",
    "        country=cmodels.Country.objects.get(name=\"Germany\"),\n",
    "        level='N'\n",
    "    )\n",
    "    if created and verbosity > 0:\n",
    "        print(\"created new object for parliament\")\n",
    "\n",
    "    pp, created = pm.ParlPeriod.objects.get_or_create(\n",
    "                                    parliament=parl,\n",
    "                                    n=wp)\n",
    "    if created and verbosity > 0:\n",
    "        print(\"created new object for legislative period\")\n",
    "\n",
    "    doc, created = pm.Document.objects.get_or_create(\n",
    "        parlperiod=pp,\n",
    "        doc_type=\"Plenarprotokoll\",\n",
    "        date=german_date(date),\n",
    "        sitting=session,\n",
    "        text_source=\"from https://www.bundestag.de/service/opendata (scans of pdfs with xml metadata)\"\n",
    "    )\n",
    "    if created:\n",
    "        print(\"created new object for plenary session document\")\n",
    "    doc.save()\n",
    "\n",
    "    doc.utterance_set.all().delete()\n",
    "\n",
    "    # parser.__iter__ yields dict with paragraphs + speaker + speaker_party + interjections (poi)\n",
    "    for contrib in parser:\n",
    "\n",
    "        if verbosity > 1:\n",
    "            print(\"saving utterance: {}\".format(contrib))\n",
    "\n",
    "        # update dictionary\n",
    "        contrib.update(base_data)\n",
    "\n",
    "        if contrib['speaker']:\n",
    "            info_dict = {'wp': wp, 'session': session, 'source_type': 'PDF/SP'}\n",
    "            speaker_party = normalize(PARTY_MEMBER_PDF.match(contrib['speaker']).group(3)).upper()\n",
    "            info_dict['party'] = speaker_party\n",
    "            per = find_person_in_db(contrib['speaker'], add_info=info_dict, verbosity=verbosity)\n",
    "        else:\n",
    "            print(\"! Warning: No speaker given, not saving the following contribution: {}\".format(contrib))\n",
    "            warnings_counter2 += 1\n",
    "            continue\n",
    "\n",
    "        if per is None:\n",
    "            print(\"! Warning: Not able to match person, not saving the following contribution: {}\".format(contrib))\n",
    "            warnings_counter2 += 1\n",
    "            continue\n",
    "\n",
    "        position = PERSON_POSITION.search(contrib['speaker'])\n",
    "\n",
    "        if position:\n",
    "            role_description = position.group(0)\n",
    "\n",
    "            speaker_role_set = pm.SpeakerRole.objects.filter(alt_names__contains=[role_description])\n",
    "            if len(speaker_role_set) < 1:\n",
    "                speaker_role = pm.SpeakerRole(name=role_description, alt_names=[role_description])\n",
    "                speaker_role.save()\n",
    "            else:\n",
    "                speaker_role = speaker_role_set.first()\n",
    "                if len(speaker_role_set) > 1:\n",
    "                    print(\"Warning: several speaker roles matching\")\n",
    "\n",
    "            ut = pm.Utterance(\n",
    "                document=doc,\n",
    "                speaker=per,\n",
    "                speaker_role=speaker_role\n",
    "            )\n",
    "        else:\n",
    "            ut = pm.Utterance(\n",
    "                document=doc,\n",
    "                speaker=per\n",
    "            )\n",
    "\n",
    "        ut.save()\n",
    "        utterance_counter += 1\n",
    "\n",
    "        for par in contrib['pars']:\n",
    "\n",
    "            if par['text']:\n",
    "                para = pm.Paragraph(\n",
    "                    utterance=ut,\n",
    "                    text=par['text'],\n",
    "                    word_count=len(par['text'].split()),\n",
    "                    char_len=len(par['text'])\n",
    "                )\n",
    "                para.save()\n",
    "                paragraph_counter += 1\n",
    "            else:\n",
    "                print(\"! Warning: Empty paragraph ({})\".format(par))\n",
    "                warnings_counter2 += 1\n",
    "                for ij in par['pois']:\n",
    "                    print(\"poi: {}\".format(ij.poitext))\n",
    "                continue\n",
    "\n",
    "            for ij in par['pois']:\n",
    "                if ij.type is None:\n",
    "                    print(\"! Warning: Ommiting interjection. Interjection type not identified for: {}\".format(ij.poitext))\n",
    "                    warnings_counter2 += 1\n",
    "                    continue\n",
    "                interjection = pm.Interjection(\n",
    "                    paragraph=para,\n",
    "                    text=ij.poitext,\n",
    "                    type=ij.type\n",
    "                )\n",
    "                interjection.save()\n",
    "                interjection_counter += 1\n",
    "\n",
    "                if ij.parties:\n",
    "                    for party_name in ij.parties.split(':'):\n",
    "                        party, created = pm.Party.objects.get_or_create(\n",
    "                            name=party_name\n",
    "                        )\n",
    "\n",
    "                        interjection.parties.add(party)\n",
    "                if ij.speakers:\n",
    "                    for person in ij.speakers:\n",
    "                        info_dict = {'wp': wp, 'session': session, 'source_type': 'PDF/POI'}\n",
    "                        per = find_person_in_db(person, add_info=info_dict, verbosity=verbosity)\n",
    "                        if per is not None:\n",
    "                            interjection.persons.add(per)\n",
    "                        else:\n",
    "                            print(\"! Warning: Speaker could not be identified\")\n",
    "                            warnings_counter2 += 1\n",
    "\n",
    "    if not parser.in_session:\n",
    "        print(\"! Error: beginning of session not found\")\n",
    "        return (1, 0)\n",
    "\n",
    "    print(\"==================================================\")\n",
    "    print(\"Summary for {}:\".format(filename))\n",
    "    print(\"number of utterances: {}\".format(utterance_counter))\n",
    "    print(\"number of paragraphs: {}\".format(paragraph_counter))\n",
    "    print(\"number of interjections: {}\".format(interjection_counter))\n",
    "    print(\"warnings in SpeechParser generator: {}\".format(parser.warnings_counter))\n",
    "    print(\"warnings in parse_transcript function: {}\".format(warnings_counter2))\n",
    "    print(\"==================================================\")\n",
    "\n",
    "    if utterance_counter <= 0:\n",
    "        return (1, 0)\n",
    "    else:\n",
    "        return (0, parser.warnings_counter + warnings_counter2)\n",
    "\n",
    "# ==========================================================================================================\n",
    "# ==========================================================================================================\n",
    "# what is this for? \n",
    "#def clear_db():\n",
    "#    database = dataset.connect(db)\n",
    "#    table = database['plpr']\n",
    "#    table.delete()\n",
    "\n",
    "# =================================================================================================================\n",
    "# =================================================================================================================\n",
    "\n",
    "\n",
    "def lines_with_one_character(file):\n",
    "\n",
    "    if isinstance(file, str):\n",
    "        # print(\"loading text from {}\".format(file))\n",
    "        with open(file) as fh:\n",
    "            text = fh.read()\n",
    "        text = text.replace(\"\\t\", \"\").split(\"\\n\")\n",
    "\n",
    "    # open file in zip archive\n",
    "    elif isinstance(file, zipfile.ZipExtFile):\n",
    "        content = file.read()\n",
    "        filename = file.name\n",
    "        if filename.endswith(\".xml\"):\n",
    "            root = ElementTree.fromstring(content)\n",
    "            text = root.find(\"TEXT\").text.replace(\"\\t\", \"\").split(\"\\n\")\n",
    "            # print(\"loading text from {}\".format(filename))\n",
    "        else:\n",
    "            print(\"filetype not xml\")\n",
    "            return None\n",
    "        file.close()\n",
    "\n",
    "    text = [line.strip() for line in text if line.strip() != '']\n",
    "    count = sum([1 for line in text if len(line) == 1])\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "# =================================================================================================================\n",
    "\n",
    "# main execution script\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    sys.stdout = Logger()\n",
    "\n",
    "    # settings for parsing\n",
    "    delete_additional_persons = False\n",
    "    delete_all = False\n",
    "    verbosity = 0\n",
    "\n",
    "    if delete_all:\n",
    "        print(\"Deleting all documents, utterances, paragraphs and interjections.\")\n",
    "        # pmodels.Person.objects.all().delete()\n",
    "        # pmodels.Parl.objects.all().delete()\n",
    "        # pmodels.ParlPeriod.objects.all().delete()\n",
    "        pm.Interjection.objects.all().delete()\n",
    "        pm.Paragraph.objects.all().delete()\n",
    "        pm.Utterance.objects.all().delete()\n",
    "        pm.Document.objects.all().delete()\n",
    "        print(\"Deletion done.\")\n",
    "\n",
    "    if delete_additional_persons:\n",
    "        print(\"Deleting all persons added from protocol parsing.\")\n",
    "        pm.Person.objects.filter(information_source__startswith=\"from protocol scraping\").delete()\n",
    "\n",
    "    document_counter = 0\n",
    "    count_errors = 0\n",
    "    count_warnings_docs = 0\n",
    "    count_warnings_sum = 0\n",
    "\n",
    "    wps = range(12, 11, -1)\n",
    "    sessions = range(13, 245)\n",
    "\n",
    "    print(\"start parsing...\")\n",
    "    for wp in wps:\n",
    "        collection = \"pp{wp:02d}-data.zip\".format(wp=wp)\n",
    "        print(collection)\n",
    "\n",
    "        archive = zipfile.ZipFile(os.path.join(data_dir, collection), 'r')\n",
    "        print(\"loading files from {}\".format(collection))\n",
    "        filelist = [fzip.filename for fzip in archive.infolist()]\n",
    "\n",
    "        for session in sessions:\n",
    "            filename = \"{wp:02d}{s:03d}.xml\".format(wp=wp, s=session)\n",
    "            if filename in filelist:\n",
    "\n",
    "                # delete old protocol\n",
    "                pm.Document.objects.filter(parlperiod__n=wp, sitting=session,\n",
    "                                           text_source=\"from https://www.bundestag.de/service/opendata \"\n",
    "                                                       \"(scans of pdfs with xml metadata)\").delete()\n",
    "                f = archive.open(filename)\n",
    "                print(f)\n",
    "                parser_errors, parser_warnings = parse_transcript(f, verbosity=verbosity)\n",
    "                count_errors += parser_errors\n",
    "                if parser_warnings > 0:\n",
    "                    count_warnings_docs += 1\n",
    "                    count_warnings_sum += parser_warnings\n",
    "\n",
    "                document_counter += 1\n",
    "                f.close()\n",
    "\n",
    "                f = archive.open(filename)\n",
    "                print(\"lines with one character: {}\".format(lines_with_one_character(f)))\n",
    "                print(\"==================================================\\n\")\n",
    "\n",
    "                f.close()\n",
    "            else:\n",
    "                print(\"{} not in archive\".format(filename))\n",
    "\n",
    "        archive.close()\n",
    "\n",
    "\n",
    "    print(\"\\n==================================================\")\n",
    "    print(\"Summary for {} documents:\".format(document_counter))\n",
    "    print(\"Documents with errors: {}\".format(count_errors))\n",
    "    print(\"Documents with warnings: {}\".format(count_warnings_docs))\n",
    "    print(\"Sum of all warnings: {}\".format(count_warnings_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/12013.xml\n"
     ]
    }
   ],
   "source": [
    "wps = range(12, 11, -1)\n",
    "sessions = range(13, 14)\n",
    "\n",
    "wp = 12\n",
    "session = 13\n",
    "\n",
    "testdata_dir = \"../data/\"\n",
    "filename = \"{wp:02d}{s:03d}.xml\".format(wp=wp, s=session)\n",
    "file = testdata_dir + filename\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading text from ../data/12013.xml\n",
      "\n",
      "Parsing transcript: 12/13, from ../data/12013.xml\n"
     ]
    }
   ],
   "source": [
    "verbosity = 1\n",
    "print(\"loading text from {}\".format(file))\n",
    "try:\n",
    "    with open(file) as fh:\n",
    "        content = fh.read()\n",
    "        text = clean_text(content)\n",
    "except UnicodeDecodeError:\n",
    "    print(\"Reloading in other encoding (windows-1252)\")\n",
    "    with open(file, encoding=\"windows-1252\") as fh:\n",
    "        content = fh.read()\n",
    "        text = clean_text(content)\n",
    "filename = file\n",
    "wp, session = file_metadata(filename)\n",
    "\n",
    "base_data = {\n",
    "    'filename': filename,\n",
    "    'sitzung': session,\n",
    "    'wahlperiode': wp\n",
    "}\n",
    "\n",
    "print(\"\\nParsing transcript: {}/{}, from {}\".format(wp, session, filename))\n",
    "utterance_counter = 0\n",
    "paragraph_counter = 0\n",
    "interjection_counter = 0\n",
    "\n",
    "parser = SpeechParser(text.split('\\n'), verbosity=verbosity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "parser.get_date\n",
    "print(parser.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= matched begin mark at line 92: Beginn: 10.01 Uhr\n",
      "= matched speaker at line 93: <re.Match object; span=(0, 31), match='Präsidentin Dr. Rita Süssmuth: '>\n",
      "= matched top mark: Nach einer interfraktionellen Vereinbarung soll die verbundene Tagesordnung erweitert werden. Die Punkte sind in der Ihnen vorliegenden Zusatzpunktliste aufgeführt:\n",
      "= setting stopword flag\n",
      "= matched top mark: Ich rufe die Tagesordnungspunkte 1 a bis 1 f auf: Überweisungen im vereinfachten Verfahren\n",
      "= setting stopword flag\n",
      "= matched header:  644\tDeutscher Bundestag – 12. Wahlperiode – 13. Sitzung. Bonn, Dienstag, den 12. März 1991\n",
      "= matched current speaker in header: Präsidentin Dr. Rita Süssmuth\n",
      "= matched top mark: Wir kommen zu Tagesordnungspunkt 1 g:\n",
      "= matched top mark: Ich rufe die Tagesordnungspunkte 2 a und 2 b sowie die Zusatzpunkte 1 bis 3 auf:\n",
      "= setting stopword flag\n",
      "interjection: speakers: [], party: , type: 7,\n",
      "interjection text: Haushaltsgesetz 1991\n",
      "= matched header:  Deutscher Bundestag – 12. Wahlperiode – 13. Sitzung. Bonn, Dienstag, den 12. März 1991\t645\n",
      "= matched current speaker in header: Präsidentin Dr. Rita Süssmuth\n",
      "= matched speaker at line 190: <re.Match object; span=(0, 49), match='Dr. Theodor Waigel, Bundesminister der Finanzen: >\n",
      "Präsidentin Dr. Rita Süssmuth\n",
      "= setting stopword flag\n",
      "interjection: speakers: [], party: linke:spd, type: 1,\n",
      "interjection text: Beifall und anhaltende Zurufe von der SPD sowie Beifall bei Abgeordneten der PDS/ Linke Liste\n",
      "interjection: speakers: ['Dr. Struck'], party: , type: 2,\n",
      "interjection text: Sie werden sich noch wundern, Herr Waigel!\n",
      "interjection: speakers: ['Wieczorek'], party: , type: 2,\n",
      "interjection text: Wir sind doch keine Schlafmützen!\n",
      "interjection: speakers: [], party: spd, type: 3,\n",
      "interjection text: Lachen und Widerspruch bei der SPD\n",
      "interjection: speakers: ['Dr. Vogel'], party: , type: 2,\n",
      "interjection text: „Steuern werden nicht erhöht! \"\n",
      "interjection: speakers: [], party: linke:spd, type: 1,\n",
      "interjection text: Lachen und Beifall bei der SPD und bei Abgeordneten der PDS/Linke Liste\n",
      "= setting stopword flag\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9197001ab11d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# utterances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcontrib\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontrib\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'speaker'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'speaker'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-33b9f4b9470c>\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpoi_match\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoi_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoi_match\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_text_and_poi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-33b9f4b9470c>\u001b[0m in \u001b[0;36mappend_text_and_poi\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m         }\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpoi_raw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' [-–—]-? '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoi_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mpoi_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPOI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoi_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mpar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pois'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoi_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbosity\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Data/plpr-scraper/scraper/parsing_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeakers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeaker\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparties\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_party_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeaker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoitext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msinfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterjection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSPEECH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Data/plpr-scraper/scraper/parsing_utils.py\u001b[0m in \u001b[0;36msearch_party_names\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPARTIES_SPLIT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mparties\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# utterances\n",
    "for contrib in parser:\n",
    "    if contrib['speaker']:\n",
    "        print(contrib['speaker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= matched top mark: Tagesordnungspunkt 1:\n",
      "= matched top mark: Tagesordnungspunkt 2:\n",
      "= matched speaker in header:  Dr. Theodor Waigel, Bundesminister BMF \t645 A\n",
      "= matched header:  Deutscher Bundestag – 12. Wahlperiode – 13. Sitzung. Bonn, Dienstag, den 12. März 1991\t643\n",
      "= matched speaker at line 93: <re.Match object; span=(0, 31), match='Präsidentin Dr. Rita Süssmuth: '>\n",
      "[<parsing_utils.POI object at 0x7f6cd3160cc0>]\n",
      "[<parsing_utils.POI object at 0x7f6cd3160780>, <parsing_utils.POI object at 0x7f6cd3160e48>]\n",
      "[<parsing_utils.POI object at 0x7f6cd3160ef0>]\n",
      "[<parsing_utils.POI object at 0x7f6cd3160da0>]\n",
      "[<parsing_utils.POI object at 0x7f6cd3160f98>]\n",
      "= matched top mark: Nach einer interfraktionellen Vereinbarung soll die verbundene Tagesordnung erweitert werden. Die Punkte sind in der Ihnen vorliegenden Zusatzpunktliste aufgeführt:\n",
      "= setting stopword flag\n",
      "= matched top mark: Ich rufe die Tagesordnungspunkte 1 a bis 1 f auf: Überweisungen im vereinfachten Verfahren\n",
      "= setting stopword flag\n",
      "= matched header:  644\tDeutscher Bundestag – 12. Wahlperiode – 13. Sitzung. Bonn, Dienstag, den 12. März 1991\n",
      "= matched current speaker in header: Präsidentin Dr. Rita Süssmuth\n",
      "= matched top mark: Wir kommen zu Tagesordnungspunkt 1 g:\n",
      "= matched top mark: Ich rufe die Tagesordnungspunkte 2 a und 2 b sowie die Zusatzpunkte 1 bis 3 auf:\n",
      "= setting stopword flag\n",
      "interjection: speakers: [], party: , type: 7,\n",
      "interjection text: Haushaltsgesetz 1991\n",
      "= matched header:  Deutscher Bundestag – 12. Wahlperiode – 13. Sitzung. Bonn, Dienstag, den 12. März 1991\t645\n",
      "= matched current speaker in header: Präsidentin Dr. Rita Süssmuth\n",
      "= matched speaker at line 190: <re.Match object; span=(0, 49), match='Dr. Theodor Waigel, Bundesminister der Finanzen: >\n",
      "[<parsing_utils.POI object at 0x7f6cd31bd080>]\n",
      "= setting stopword flag\n",
      "interjection: speakers: [], party: linke:spd, type: 1,\n",
      "interjection text: Beifall und anhaltende Zurufe von der SPD sowie Beifall bei Abgeordneten der PDS/ Linke Liste\n",
      "interjection: speakers: ['Dr. Struck'], party: , type: 2,\n",
      "interjection text: Sie werden sich noch wundern, Herr Waigel!\n",
      "interjection: speakers: ['Wieczorek'], party: , type: 2,\n",
      "interjection text: Wir sind doch keine Schlafmützen!\n",
      "interjection: speakers: [], party: spd, type: 3,\n",
      "interjection text: Lachen und Widerspruch bei der SPD\n",
      "interjection: speakers: ['Dr. Vogel'], party: , type: 2,\n",
      "interjection text: „Steuern werden nicht erhöht! \"\n",
      "interjection: speakers: [], party: linke:spd, type: 1,\n",
      "interjection text: Lachen und Beifall bei der SPD und bei Abgeordneten der PDS/Linke Liste\n",
      "= setting stopword flag\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-e3d7d5c2b1bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# interjections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcontrib\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontrib\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pars'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pois'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-33b9f4b9470c>\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpoi_match\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoi_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoi_match\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_text_and_poi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-33b9f4b9470c>\u001b[0m in \u001b[0;36mappend_text_and_poi\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m         }\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpoi_raw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' [-–—]-? '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoi_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mpoi_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPOI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoi_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mpar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pois'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoi_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbosity\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Data/plpr-scraper/scraper/parsing_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeakers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeaker\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparties\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_party_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeaker\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoitext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msinfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterjection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSPEECH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Data/plpr-scraper/scraper/parsing_utils.py\u001b[0m in \u001b[0;36msearch_party_names\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPARTIES_SPLIT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mparties\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# interjections\n",
    "for contrib in parser:\n",
    "    for par in contrib['pars']:\n",
    "        print(par['pois'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"Dr. Vogel [SPD]:\"\n",
    "test_string2 = 'Wieczorek [Duisburg] [SPD]'\n",
    "#test_string = normalize(test_string)\n",
    "test_party = test_string.strip().split(' ')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[SPD]:'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_party2 = 'F.D.P.'\n",
    "test_person = 'Rühe (CDU/CSU)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTIES_REGEX_PDF = {\n",
    "    'cducsu': re.compile(' ?cdu ?(csu)?'),\n",
    "    'spd': re.compile(' ?spd'),\n",
    "    'linke': re.compile(' ?(die|der|den) linken?| pds'),\n",
    "    'fdp': re.compile(' ?fdp|F.D.P.'),\n",
    "    'gruene': re.compile(' ?bund ?nis\\-?(ses)? ?90 die gru ?nen'),\n",
    "    'afd': re.compile(' ?AfD')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_party = ' '\n",
    "for party, rex in PARTIES_REGEX.items():\n",
    "    if rex.findall(test_party):\n",
    "        person_party = party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def search_person_party(text):\n",
    "    \"\"\"\n",
    "    returns the party of a speaker from an input string (from raw text)\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return\n",
    "    text = normalize(text)\n",
    "    # identify correct group of text in name\n",
    "    \n",
    "    # find party\n",
    "    for party, rex in PARTIES_REGEX.items():\n",
    "        if rex.findall(text):\n",
    "            person_party = party\n",
    "            return person_party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cducsu'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_person_party(test_person)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group 1 match\n",
    "(.{4,50}?)\n",
    "# group 2 match\n",
    "([\\[\\(].*[\\]\\)])?\n",
    "#group 3 match\n",
    "([\\[\\(].*[\\]\\)])?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups name, ortszusatz (if any), and party of a speaker\n",
    "PARTY_MEMBER_PDF = re.compile('(.{4,50}?([\\[\\(].*[\\]\\)])?\\s?([\\[\\(].*[\\]\\)]))?\\s?:?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = PARTY_MEMBER_PDF.match(test_string).group(3)\n",
    "speaker_party = normalize(test_result)\n",
    "speaker_party = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dr. Vogel '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = clean_text(test_string)\n",
    "name = INHYPHEN.sub(r'\\1\\2', name)\n",
    "name = name.replace('\\n', ' ')\n",
    "name = name.rstrip(':') #new\n",
    "name = NAME_REMOVE.sub('', name)\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SPD'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PERSON_PARTY.match(test_string.rstrip(':')).group(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing find_person_in_db modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Person: Dr. Hans-Jochen Vogel>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# utterances\n",
    "# this works! now to implement into parser \n",
    "person = test_string\n",
    "info_dict = {'wp': wp, 'session': session, 'source_type': 'PDF/SP'}\n",
    "speaker_party = normalize(PARTY_MEMBER_PDF.match(person).group(3))\n",
    "info_dict['party'] = speaker_party\n",
    "per = find_person_in_db(person, add_info=info_dict, verbosity=verbosity) \n",
    "per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spd'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_dict['party']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! Warning: Could not distinguish between persons!\n",
      "For name string: Dr. Vogel \n",
      "first name: , surname: Vogel\n",
      "title: Dr., party: None, position: None, ortszusatz: None\n",
      "Query: <QuerySet [<Person: Dr. Hans-Jochen Vogel>, <Person: Dr. Vogel>, <Person: Dr. Hans Vogel>, <Person: Dr. HansJochen Vogel>, <Person: Dr. Hans-Joachim Vogel>]>\n",
      "Clean names: ['Dr. Hans-Jochen Vogel', 'Dr. Vogel', 'Dr. Hans Vogel', 'Dr. HansJochen Vogel', 'Dr. Hans-Joachim Vogel']\n",
      "Taking first entry of ambiguous results\n"
     ]
    }
   ],
   "source": [
    "# interjections\n",
    "info_dict = {'wp': wp, 'session': session, 'source_type': 'PDF/POI'}\n",
    "per = find_person_in_db(person, add_info=info_dict, verbosity=verbosity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
